{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4988daf4-346a-4a15-b8f8-4802ccdfb87b"
    }
   },
   "source": [
    "# Searchlights <a name=\"searchlights\"></a>\n",
    "\n",
    "In univariate analyses there is a distinction between whole-brain and [ROI-based](https://doi.org/10.1093/scan/nsm006) analyses. When done correctly, whole-brain univariate analyses allow you to identify, in an unbiased and data-driven way, regions of the brain that differ between experimental conditions. Compare this to what we have done with multivariate classification: up to this point we have taken ROIs (e.g., FFA, PPA, even the whole brain) and looked at the pattern of activity across voxels. Using these procedures we have been able to determine whether these voxels collectively contain information about different conditions. However, throughout these analyses it has been unclear whether a subset of these voxels have been driving classification accuracy. Although we could use the weights to indicate which voxels are important, weights alone do not always convey the influence of individual voxels. In other words, we have been unable to say with certainty *where* in the brain there are representations that distinguish between conditions. \n",
    "\n",
    "A searchlight is a spatial moving window that exhaustively searches the brain in order to localize representations. Running a searchlight is computationally intensive because it involves running a separate multivariate analysis for every voxel in the brain. Imagine doing your feature selection, hyper-parameter optimization, and cross-validation 100,000 times or more. Fortunately, [BrainIAK](http://brainiak.org/) (see introductory [article](https://apertureneuropub.cloud68.co/articles/42/)) contains a procedure that efficiently carves up brain data into appropriate chunks and distributes them across the computational resources available. \n",
    "\n",
    "For more information on the searchlight technique, read this comprehensive [NeuroImage Comment](https://doi.org/10.1016/j.neuroimage.2013.03.041) which also includes citations to landmark papers on the topic. \n",
    "\n",
    "## Goal of this script\n",
    "\n",
    "1. Learn how to perform a whole-brain searchlight.  \n",
    "2. Learn how to replace the kernel used inside the searchlight.  \n",
    "3. Use batch scripts, SLURM, and MPI to run searchlight on compute clusters.\n",
    "4. Run searchlight on a face-scene dataset.\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Searchlight workflow](#sl_wf)  \n",
    ">1.1 [Data preparation](#data_prep)  \n",
    ">1.2 [Executing the searchlight workflow](#exe_wf)   \n",
    ">1.3 [Executing on multiple subjects](#multi_subj)\n",
    "2. [Running searchlight analyses on a cluster](#submitting_searchlights)  \n",
    ">2.1 [Distributing jobs with parallel computing](#ranks)\n",
    "\n",
    "Exercises\n",
    "\n",
    ">[1](#ex1)   [2](#ex2)  [3](#ex3)  [4](#ex4)  [5](#ex5)  [6](#ex6)  [7](#ex7)  [8](#ex8)  [9](#ex9) [10](#ex10) [11](#ex11) [12](#ex12)\n",
    "\n",
    "[Novel Contribution](#novel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset <a id=\"data-set\"></a> \n",
    "\n",
    "For this script we will use the face/scene dataset from [Wang et al. (2015)](https://doi.org/10.1016/j.jneumeth.2015.05.012), who in turn used localizer data from [Turk-Browne et al. (2012)](https://doi.org/10.1523/JNEUROSCI.0942-12.2012).\n",
    "\n",
    "Localizer details from Turk-Browne et al. (2012):\n",
    "\n",
    "    Subjects also completed a functional localizer run lasting 6 minutes, 6 seconds. ... The localizer run alternated between six repetitions each of scene and face blocks (Turk-Browne et al., 2010). Blocks contained 12 stimuli presented for 500 ms every 1500 ms. The 18 s of stimulation was followed by 12 s of fixation before the next block. Subjects made the same indoor/outdoor judgment for scenes, and a male/female judgment for faces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Searchlight workflow <a id=\"sl_wf\"></a>\n",
    "\n",
    "Running a searchlight is computationally intensive and complex, involving multiple steps. To show how the searchlight functionality in BrainIAK works, we will walk through the steps needed to perform a simple searchlight that can be run in this notebook before moving onto a real example that requires submitting the job to a cluster, where it will have access to more computational resources. This simple workflow takes the following steps:\n",
    "\n",
    ">1. [Data preparation](#data_prep)\n",
    ">2. [Set the searchlight parameters](#set_param)\n",
    ">3. [Create the searchlight object](#create_obj)\n",
    ">4. [Create our classification kernel](#create_kernel)\n",
    ">5. [Execute the searchlight](#exec_sl)\n",
    "\n",
    "\n",
    "However, before we start, there are a few things that you should know about searchlights. Think of a searchlight as a processing step in which you pull out a certain size chunk of your data and then perform a kernel operation (specified by a function you write). You can use any kernel on this chunk of data interchangeably. Critically, **the searchlight function does not specify the analysis you want to perform; all it does is carve up the data.**\n",
    "\n",
    "**Computational demand and parallelization**\n",
    "\n",
    "As mentioned before, searchlights are computationally intensive, so we need to be aware of the kind of computational burden this analysis imposes. With each subject, we want to perform the operation thousands of times (once for each of their brain voxels). If we were to run this serially (meaning, a single searchlight at a time), *even if each operation only took 1 s to complete*, the analysis would take multiple days for *just one* participant. With nested cross-validation or other recursive steps, the full analysis could take months or years -- far beyond the due date for this assignment!\n",
    "\n",
    "Fortunately, the searchlight function in BrainIAK intelligently parallelizes your data to give you a considerable and scalable speed boost. Parallelization is the idea that when two or more computational tasks can be completed independently (because they don't interact in any way), these tasks can be run simultaneously on different cores. Note: we refer to cores as our unit of serial processing, although there are other parallelizations available within-core, such as threads or even multiple instructions within thread. The nice thing about parallelization is that it is scalable: in general, a job parallelized across 10 cores will run up to 10 times faster than on one core. For reference, your computer likely has 2, 4, or more cores, so you could speed up processing if you recruited all of these resources (and shut down all other types of background processing). \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "So remember, the two main things that determine the speed of a searchlight: **the kernel algorithm** and the **amount of parallelization**.\n",
    "</div>\n",
    "\n",
    "**How does the BrainIAK searchlight work?**\n",
    "\n",
    "To analyze data efficiently, it is important to understand how the Searchlight function in BrainIAK works. You must provide the function with a numpy volume of 4D data and a binary mask of brain voxels, which tells the function which of the voxels in that 4D volume correspond to voxels inside the brain. The searchlight code simply searches for every voxel in the mask that is equal to 1 and then centers a searchlight around this. This means that for every value of 1 in your mask, the searchlight function will apply your kernel function to the corresponding voxel (and those around it) in the 4D volume. Hence when writing the kernel you need to keep in mind that the input data the function receives are not the size of the original data but instead the size of the searchlight radius. In other words, you might give the searchlight function a brain and mask volume that is 64x64x64 but each kernel operation only runs on a data and mask volume that is 3x3x3 (or whatever your searchlight radius is). \n",
    "\n",
    "**How to start using a searchlight? Small!**\n",
    "\n",
    "When getting used to searchlights it is encouraged that you scale up your code gently. This is to prevent the possibility that you run a searchlight that takes a very long time to finish only to find there was a small error (which happens a lot). Instead it is better to write a simple function that runs quickly so you can check if your code works properly before scaling up. A simple workflow for testing your searchlight (and estimating how long your searchlight would take on a bigger dataset) would be this:\n",
    "\n",
    "1. Create a brain mask with one voxel and run the searchlight interactively (like we are doing now, using a notebook) to check whether the code works.\n",
    "2. Use timestamps to extract the execution time.  \n",
    "3. Print the number of voxels that are passed to the searchlight kernel (the one voxel and its neighbors).  \n",
    "4. Run the searchlight as a job on the smallest unit of real data you have (a single run or single participant).\n",
    "5. Check the runtime and memory usage of this searchlight.\n",
    "\n",
    "Taking our own advice, we are going to write a searchlight script below to perform a very simple searchlight. In fact, we are only going to run this searchlight on one voxel in the brain so that we can see whether our code is working and how long each kernel operation would take. After we have familiarized ourselves with the way searchlights work on a small scale, we will then graduate to full-scale analyses using batch scripts and cluster computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "# Import libraries\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import glob\n",
    "from nilearn import plotting\n",
    "from brainiak.searchlight.searchlight import Searchlight\n",
    "from brainiak.fcma.preprocessing import prepare_searchlight_mvpa_data\n",
    "from brainiak import io\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Set printing precision\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style = 'white', context='notebook', rc={\"lines.linewidth\": 2.5})\n",
    "sns.set(palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data preparation <a id=\"data_prep\"></a>\n",
    "Prepare data for a single participant using a similar pipeline to what we've been using previously. The critical change needed here is the shape of the data: in the past we wanted a time by voxel matrix, but the input to BrainIAK's searchlight is a 4D volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Helper functions <a id=\"helper\"> </a>\n",
    "\n",
    "To make it easy for you to achieve the main goals of this notebook, we have created helper functions that perform data extraction from nifti file data, format them into arrays, and prepare the data for the searchlight function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fs_data_dir, results_path\n",
    "\n",
    "print(f'data dir = {fs_data_dir}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the raw data and save it to a location \n",
    "def preprocess_and_save_data(fs_data_dir, output_path):\n",
    "    \n",
    "    # Set paths\n",
    "    suffix = 'bet.nii.gz'\n",
    "    wb_mask_file =  f'{fs_data_dir}/mask.nii.gz' # whole brain mask\n",
    "    fs_epoch_file = f'{fs_data_dir}/fs_epoch_labels.npy' # face/scene epoch labels   \n",
    "    \n",
    "    # Load in the image files from directory `fs_data_dir` that end with a given suffix.\n",
    "    images = io.load_images_from_dir(in_dir=fs_data_dir, suffix=suffix)\n",
    "    \n",
    "    # Grab the filenames that correspond with these images.\n",
    "    image_file_names = sorted(glob.glob(f\"{fs_data_dir}/*{suffix}\")) \n",
    "    \n",
    "    # Load epoch file.\n",
    "    epoch_list = io.load_labels(fs_epoch_file)\n",
    "    \n",
    "    # Get some variable information from the epoch list\n",
    "    n_subjects, n_conditions, n_epochs, n_TRs = np.shape(epoch_list)\n",
    "    \n",
    "    # Normalize and format the data.\n",
    "    bold_data_raw, labels = prepare_searchlight_mvpa_data(images, epoch_list)\n",
    "    labels = np.array(labels) \n",
    "    # Note: `labels` and `bold_data_raw` are stacked events for all subjects.\n",
    "    # That's why len(labels) = n_subjects * n_epochs\n",
    "    \n",
    "    # Load the whole brain mask file and determine which voxels are in the brain. \n",
    "    whole_brain_mask = nib.load(wb_mask_file).get_fdata()\n",
    "    coords = np.where(whole_brain_mask !=0)[0]  # Where are the nonzero values?\n",
    "    \n",
    "    # Copy the whole brain mask file.\n",
    "    copyfile(wb_mask_file, f\"{output_path}/wb_mask.nii.gz\")\n",
    "    \n",
    "    # Loop through all the subjects.\n",
    "    # We now are going to break the data and labels down for each subject and save independently.\n",
    "    for sub_id in range(n_subjects): \n",
    "        sub = f'sub-{sub_id+1:02d}'\n",
    "        \n",
    "        # Load in the NIfTI file so we can get some information about the image.\n",
    "        nii = nib.load(image_file_names[sub_id])\n",
    "        affine  = nii.affine  # Like the affine, which defines the orientation of the data.\n",
    "        dimensions = nii.header.get_zooms() # And dimensions of the voxels in millimeters.\n",
    "        \n",
    "        # Extract the bold data and corresponding labels for this subject.\n",
    "        this_subject_indices = np.arange(sub_id * n_epochs, (sub_id + 1) * n_epochs)\n",
    "        data_i = bold_data_raw[:, :, :, this_subject_indices]\n",
    "        label_i = labels[this_subject_indices]\n",
    "\n",
    "        # Save data in one directory per subject.\n",
    "        output_dir = os.path.join(output_path, sub)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        bold_nii = nib.Nifti1Image(data_i, affine)\n",
    "        bold_nii.header.set_zooms(dimensions)\n",
    "        nib.save(bold_nii, f'{output_dir}/data.nii.gz') \n",
    "        np.save(f'{output_dir}/label.npy', label_i)\n",
    "        print(f'saved {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and prepare data for one subject.\n",
    "def load_fs_data(data_path, sub_id, mask=None):\n",
    "    # Find the file path\n",
    "    sub = f'sub-{sub_id:02d}'\n",
    "    data_dir = f'{data_path}/{sub}'\n",
    "    data_file = f'{data_dir}/data.nii.gz'\n",
    "    label_file =  f'{data_dir}/label.npy'\n",
    "    \n",
    "    if not mask:\n",
    "        mask = 'wb'\n",
    "    mask_file = f'{data_path}/{mask}_mask.nii.gz'\n",
    "    \n",
    "    # Load bold data and some header information so that we can save searchlight results later.\n",
    "    nii = nib.load(data_file)\n",
    "    bold_data = nii.get_fdata()\n",
    "    affine_mat = nii.affine\n",
    "    dimensions = nii.header.get_zooms() \n",
    "    \n",
    "    # Load label\n",
    "    label = np.load(label_file)\n",
    "\n",
    "    # Load mask\n",
    "    brain_mask = nib.load(mask_file).get_fdata()\n",
    "\n",
    "    return bold_data, label, brain_mask, affine_mat, dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex1\"></a>\n",
    "**Exercise 1:** Why does the input to a searchlight analysis need to be 4D rather than 2D? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Create a directory structure<a id=\"helper\"> </a>\n",
    "\n",
    "We strongly recommend storing the data and results generated by the analysis in separate directories from the raw data (here we have enforced this by making the raw data directory read-only). On clusters these kinds of intermediate products should be stored on high-speed `scratch` disks. Typically, once your analysis is finished, you would copy the results and any important intermediate steps back to more permanent storage like your home directory or a project directory.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Just like a whiteboard is wiped regularly, scratch storage is typically erased after a month or two. Moreover, scratch space is not backed up, so you could commit your work regularly, at a minimum after every session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify the path to a useable `scratch` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your scratch directory - all data here gets deleted after 60 days\n",
    "scratch_folder =  os.path.expanduser(\"~/palmer_scratch/\")\n",
    "\n",
    "# Set up directories\n",
    "# Preprocessed data path\n",
    "data_path = os.path.expanduser(scratch_folder + '/brainiak_results/searchlight_data')\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "    \n",
    "# Make directory to save results\n",
    "output_dir = os.path.expanduser(scratch_folder + '/brainiak_results/searchlight_results')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that if you change the directory names here you will need to update them in the searchlight.py script later on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Loading, normalizing, and saving the data<a id=\"helper\"> </a>\n",
    "\n",
    "The function `preprocess_and_save_data` will normalize and save the data to `searchlight_data`. Some important points to note about this function.  \n",
    ">1. `io.load_images_from_dir(data_dir, suffix)` : This function creates a list of files that need to be loaded. You can load multiple subjects data at once.  \n",
    ">2. `io.load_boolean_mask`: Loads a whole brain mask.  \n",
    ">3. `prepare_searchlight_mvpa_data`: Normalizes and concatenates data from all subjects into a list of arrays. It also concatenates the labels from each subject. Some analysis methods use data across subjects and this step ensures that you are ready for either option.  \n",
    ">4. `epoch_list`: Specifies the timing of blocks from the different experimental conditions [participant,condition,block index,time]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before preprocessing the data, let's take a look at the structure of the raw epoch file.\n",
    "fs_epoch_file = f'{fs_data_dir}/fs_epoch_labels.npy'\n",
    "\n",
    "# Read in the epoch data from \"fs_epoch_labels.npy\"\n",
    "epoch_label = np.load(fs_epoch_file)\n",
    "\n",
    "# Inspect the shape of the numpy array. Should be (subjects, conditions, epoch_ids, TRs)\n",
    "print(epoch_label.shape)\n",
    "\n",
    "# Display the first 3 epochs separately as TR sequence in the first participant \n",
    "# Plot both conditions in the same subplot but with different colors\n",
    "fig, ax = plt.subplots(3,1, figsize = (9, 6))\n",
    "for ep in range(3):\n",
    "    ax[ep].plot(epoch_label[0,0,ep,:],'r') # red for condition 0\n",
    "    ax[ep].plot(epoch_label[0,1,ep,:],'b') # blue for condition 1\n",
    "    ax[ep].set_xlabel(\"TR\")\n",
    "    ax[ep].set_title('epoch % s' % ep, fontsize=14)\n",
    "    ax[ep].tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax[ep].set_yticks([0,1])\n",
    "    ax[ep].set_ylabel(\"Condition\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call data preprocessing script. You only need to run this line once, even if you have re-run earlier cells.\n",
    "# We recommend commenting this out after you run it for the first time!\n",
    "preprocess_and_save_data(fs_data_dir, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the first subject\n",
    "sub_id = 1\n",
    "bold_vol, labels, whole_brain_mask, affine, dimensions = load_fs_data(data_path, sub_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong> What if I want to run searchlights in the individual subject space? </strong>\n",
    "<br>\n",
    "The current face-scene dataset used in this notebook is aligned to standard MNI space. Hence, it is fine to apply the same whole-brain mask to all subjects. Often, we would like to run searchlights in the individual subject space, for example, in previous notebooks we ran decoding on each individual subject's data (that had multiple runs). If that is the case, you can just load subject 4D data in, name it `bold_vol`, along with the subject specific mask named `whole_brain_mask`. We have provided a batch script `./07-searchlight/searchlight_single_subject.py` that implements within-subject searchlight analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex2\"></a>\n",
    "**Exercise 2.** What do the variables `affine` and `dimensions` (returned by `load_fs_data`) represent?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A**:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## 1.2 Executing the searchlight workflow<a id=\"exe_wf\"></a>\n",
    "### 1.2.1 Set searchlight parameters <a id=\"set_param\"></a>\n",
    "\n",
    "To run the [searchlight](http://brainiak.org/docs/brainiak.searchlight.html) function in BrainIAK you need the following parameters:  \n",
    "\n",
    "1. **data** = The brain data as a 4D volume.  \n",
    "2. **mask** = A binary mask specifying the \"center\" voxels in the brain around which you want to perform searchlight analyses. A searchlight will be drawn around every voxel in this mask with the value of 1. Hence, if you chose to use the whole-brain mask as the mask for the searchlight procedure, the searchlight may include voxels outside of your mask when the \"center\" voxel is at the border of the mask. It is up to you to decide whether then to include these results.  \n",
    "3. **bcvar** = An additional variable which can be a list, numpy array, dictionary, etc. you want to **b**[road]**c**[ast] in your searchlight kernel. For instance, you might want to broadcast the condition labels so that you can determine to which condition each 3D volume corresponds. If you don't need to broadcast anything, e.g, when doing RSA, set this to 'None'.  \n",
    "4. **sl_rad** = The size of the searchlight's radius, excluding the center voxel. This means the total volume size of the searchlight, if using a cube, is defined as: ((2 * sl_rad) + 1) ^ 3.  \n",
    "5. **max_blk_edge** = When the searchlight function carves the data up into chunks, it doesn't distribute only a single searchlight's worth of data. Instead, it creates a block of data, with the edge length specified by this variable, which determines the number of searchlights to run within a job.  \n",
    "6. **pool_size** = Maximum number of cores running on a block (typically 1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex3\"></a>\n",
    "**Exercise 3:** Searchlights don't need to be cubes. What other shape(s) does BrainIAK support? How do you specify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mask of only one arbitrary voxel\n",
    "small_mask = np.zeros(whole_brain_mask.shape)\n",
    "target_voxel = (16,20,16)\n",
    "small_mask[target_voxel] = 1\n",
    "\n",
    "# Preset the variables to be used in the searchlight\n",
    "data = bold_vol\n",
    "mask = small_mask\n",
    "bcvar = labels\n",
    "sl_rad = 1\n",
    "max_blk_edge = 5\n",
    "pool_size = 1\n",
    "\n",
    "# We will get back to these commands once we finish running a simple searchlight. \n",
    "# comm = MPI.COMM_WORLD\n",
    "# rank = comm.rank\n",
    "# size = comm.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Create Searchlight Object  <a id=\"create_obj\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the searchlight object\n",
    "sl = Searchlight(sl_rad=sl_rad, max_blk_edge=max_blk_edge)\n",
    "print(\"Setup searchlight inputs\")\n",
    "print(f\"Input data shape: {data.shape}\")\n",
    "print(f\"Input mask shape: {mask.shape}\")\n",
    "\n",
    "# Distribute the information to the searchlights (preparing it to run)\n",
    "sl.distribute([data], mask)\n",
    "\n",
    "# Data that is needed for all searchlights is sent to all cores via the sl.broadcast function. In this example, we are sending the labels for classification to all searchlights.\n",
    "sl.broadcast(bcvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Define the function (a.k.a. \"kernel\") that needs to be computed <a id=\"create_kernel\"></a>\n",
    "\n",
    "Every searchlight needs a function to run called a \"kernel.\" This is the function that you want to measure/classify your data with. This could perform classification, RSA, or any other computation that you wish.\n",
    "\n",
    "*Nota bene:* The word \"kernel\" is used in multiple mathematical/computational contexts. In this notebook we use kernel to represent the function that is passed to the searchlight object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a SVM kernel function\n",
    "def calc_svm(data, sl_mask, myrad, bcvar):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        data: a list of 4D arrays containing data of voxels within a searchlight (could be one subject or multiple subjects' data)\n",
    "        sl_mask: a binary mask specifying the voxels at the centers of the searchlights.\n",
    "        myrad: (integer) the radius of the searchlight\n",
    "        bcvar: the data broadcast to each searchlight, used in the kernel calculation (here, the labels for SVC)\n",
    "        \n",
    "    Returns:\n",
    "        accuracy: average classification accuracy after cross-validation\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Pull out the data.\n",
    "    data4D = data[0]\n",
    "    labels = bcvar # This is the data broadcasted to all your searchlights.\n",
    "    \n",
    "    bolddata_sl = data4D.reshape(sl_mask.shape[0] * sl_mask.shape[1] * sl_mask.shape[2], data[0].shape[3]).T\n",
    "\n",
    "    # Check if the number of voxels is what you expect.\n",
    "    if verbose:\n",
    "        print(f\"Searchlight data shape: {data4D.shape}\")\n",
    "        print(f\"Searchlight data shape after reshaping: {bolddata_sl.shape}\")\n",
    "        print(f\"Searchlight mask shape: {sl_mask.shape}\")\n",
    "        print(f\"Searchlight mask (note that the center equals 1) {sl_mask}\")\n",
    "    \n",
    "    t1 = time.time()\n",
    "    clf = SVC(kernel='linear', C=1)\n",
    "    scores = cross_val_score(clf, bolddata_sl, labels, cv=3)\n",
    "    accuracy = scores.mean()\n",
    "    t2 = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Kernel duration: {np.round((t2-t1),2)}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2.4 Execute the searchlight <a id=\"exec_sl\"></a>\n",
    "\n",
    "We execute the searchlight and save the results in brain space. Each searchlight result is assigned to the voxel that the searchlight is centered on. This array is then saved to a nifti volume using the following:\n",
    "\n",
    "> `nib.Nifti1Image`: Created a nifti volume with the same size and in the same space (e.g. MNI) as the subject data.  \n",
    "> `.header`: This gets header information from the subject data.  \n",
    "> `.set_zooms`: Sets the voxel dimensions (e.g 1.5mm x 1.5mm x 1.5mm).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the clock to time searchlight.\n",
    "begin_time = time.time()\n",
    "\n",
    "print(\"Begin Searchlight\")\n",
    "verbose=True\n",
    "sl_result = sl.run_searchlight(calc_svm, pool_size=pool_size)\n",
    "verbose=False # if you want to print out all that information when you next run the searchlight kernel, you can set verbose to True\n",
    "print(\"End Searchlight\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print outputs.\n",
    "print(\"Summarize searchlight results:\")\n",
    "print(f\"Number of searchlights run: {len(sl_result[mask==1])}\")\n",
    "print(f\"Accuracy for each kernel function: {sl_result[mask==1].astype('double')}\")\n",
    "print(f'Total searchlight duration (including start up time): {np.round((end_time - begin_time),2)}')\n",
    "\n",
    "# Save the results to a .nii file.\n",
    "output_name = os.path.join(output_dir, f'subj{sub_id}_SL_result.nii.gz')\n",
    "sl_result = sl_result.astype('double')  # Convert the output into a precision format that can be used by other applications.\n",
    "sl_result[np.isnan(sl_result)] = 0  # Exchange nans with zero to ensure compatibility with other applications.\n",
    "sl_nii = nib.Nifti1Image(sl_result, affine)  # Create the volume image.\n",
    "sl_nii.header.set_zooms(dimensions[:3]) # Set zooms in header.\n",
    "nib.save(sl_nii, output_name)  # Save the volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex4\"></a>\n",
    "**Exercise 4:** How large would the searchlight be if you set `sl_rad = 4`? (Please still use `sl_rad = 1` in the following exercises, we just want you to state the answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"ex5\"></a>\n",
    "**Exercise 5:** Using the same parameters as above, around how long would it take to run the searchlight analysis on the whole brain?   \n",
    "*Note: you'd only run the searchlight analysis on voxels that are in the brain.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex6\"></a>\n",
    "**Exercise 6:** Scale up the ``small_mask`` variable to select 64 brain voxels (a 4x4x4 mini-brain) *including* the voxel in the example above. Call this mask ``my_small_mask``. *Hint: You are not changing the searchlight radius, just the brain mask that defines the searchlight centers.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "\n",
    "\n",
    "# Validate your code by uncommenting the following line\n",
    "# np.sum(my_small_mask) == 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"ex7\"></a>\n",
    "**Exercise 7:** Create a new function called `my_calc_svm` by modifying the `calc_svm` function in two ways:\n",
    "\n",
    "1. Run only if there are at least 12 \"brain\" voxels from the mask in the 27-voxel searchlight centered on each of the 64 voxels specified in ``my_small_mask``, otherwise return `np.nan`. \n",
    "    * This could be useful if you wanted to ignore searchlights near the edge or corner of the brain where only part of the searchlight is in the brain.   \n",
    "\n",
    "2. Perform nested cross-validation over the cost parameters for the linear SVM kernel. For reference, use the code you made for the classification notebook in week 5. \n",
    "    * In your inner loop, evaluate costs `C=[0.01, 0.1, 1, 10]` to select the best `C` value. \n",
    "    * Since we do not have run_ids to define outer cross-validation folds as in previous notebooks, use [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) to perform the cross-validation with `n_splits=3` in both the inner and outer loops.\n",
    "\n",
    "**Make sure to include docstring commenting for your function** You can remove print statements from your function if you desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ex8\"></a>\n",
    "**Exercise 8:** Start with the code in 1.2.1 - 1.2.4 and implement your changes from Exercises 6 & 7 to run searchlight analysis on the new small mask. \n",
    "\n",
    "To recap, the changes should include:\n",
    "1. Your new `my_small_mask`, which selects a 4x4x4 sub-volume of 64 voxels. \n",
    "2. Your new `my_calc_svm` kernel function, which only runs if there are at least 12 brain voxels from the mask in the searchlight and performs nested cross-validation. \n",
    "\n",
    "You will need to:\n",
    "- Use the defined `my_small_mask`.\n",
    "- Set the searchlight variables `data`,`mask`,`bcvar`,`sl_rad`,`max_blk_edge`, and `pool_size` (Make sure you leave `sl_rad = 1`!)  \n",
    "- Create the searchlight object with these parameters.\n",
    "- Distribute the necessary information to the searchlights.\n",
    "- Broadcast necessary variables to the searchlight.\n",
    "- Run the searchlight with your new `my_calc_svm` kernel function and `pool_size`. Make sure to time how long this step takes! \n",
    "- Save the result of `run_searchlight` in `my_sl_result`\n",
    "- Print out the average/std accuracy of `run_searchlight` and the time elapsed using the following snippets:  \n",
    "    - `print(f\"Average searchlight accuracy: \", np.nanmean(my_sl_result[np.where(my_sl_result)]))`\n",
    "    - `print(f\"Standard deviation searchlight accuracy: \",np.nanstd(my_sl_result[mask!=0]))`\n",
    "    - `print(\"Time elapsed: \", (t1-t0) / 60) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the data**\n",
    "\n",
    "As you have seen, the output of the searchlight analysis is a 3D volume with the outputs of your kernel for each voxel it was centered on. The [nilearn package](http://nilearn.github.io/plotting/index.html) has multiple plotting options to allow you to view your data within python. This is good for quick visualizations but can be buggy and is not great for exploration. You can also view the data in other packages such as FSL or AFNI. If you followed the steps above, you likely won't see many bright spots because we only ran the searchlight on a tiny part of the brain. But if you look at the data in fslview, or try to plot the output image with matplotlib, you should see non-zero values at the coordinates of your small mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Executing on multiple subjects <a id=\"multi_subj\"></a>\n",
    "\n",
    "So far we have loaded data for one subject and performed within-subject analysis, similar to what we have done in notebooks 1-5 (but in standard MNI space). You can also perform searchlight analysis on a group of subjects. This requires making a list of subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3 subjects\n",
    "subjects = [1,2,3]\n",
    "all_bold = [] # Each subject's data is an element in the list. BrainIAK will handle this format.\n",
    "all_labels = []\n",
    "all_affines = []\n",
    "all_dimensions = []\n",
    "for sub_id in subjects:\n",
    "    bold_vol, labels, whole_brain_mask, affine, dimension = load_fs_data(data_path, sub_id)\n",
    "    all_bold.append(bold_vol)\n",
    "    all_labels.append(labels)\n",
    "    all_affines.append(affine)\n",
    "    all_dimensions.append(dimension)\n",
    "\n",
    "# Make a mask of a 4 x 4 x 4 region\n",
    "target_voxel = (16, 20, 16)\n",
    "small_mask = np.zeros(whole_brain_mask.shape)\n",
    "x,y,z = target_voxel\n",
    "small_mask[x-2:x+2, y-2:y+2, z-2:z+2] = 1\n",
    "\n",
    "# Preset the variables\n",
    "data = all_bold\n",
    "mask = small_mask\n",
    "bcvar = all_labels\n",
    "sl_rad = 1\n",
    "max_blk_edge = 5\n",
    "pool_size = 1\n",
    "\n",
    "# Start the clock to time searchlight.\n",
    "begin_time = time.time()\n",
    "\n",
    "# Create the searchlight object.\n",
    "sl = Searchlight(sl_rad=sl_rad,max_blk_edge=max_blk_edge)\n",
    "print(\"Setup searchlight inputs:\")\n",
    "print(f\"Number of subjects: {len(data)}\")\n",
    "print(f\"Input data shape: {data[0].shape}\")\n",
    "print(f\"Input mask shape: {mask.shape}\")\n",
    "\n",
    "\n",
    "# Distribute the information to the searchlights (preparing it to run).\n",
    "sl.distribute(data, mask)\n",
    "# Data that is needed for all searchlights is sent to all cores via the sl.broadcast function. \n",
    "# In this example, we are sending the labels for classification to all searchlights.\n",
    "sl.broadcast(bcvar)\n",
    "\n",
    "# Set up the kernel, in this case an SVM\n",
    "def calc_svm(data, sl_mask, myrad, bcvar):\n",
    "    accuracy = []\n",
    "    # Loop over subject: \n",
    "    for idx in range(len(data)):\n",
    "        # Pull out the data\n",
    "        data4D = data[idx]\n",
    "        labels = bcvar[idx]\n",
    "        bolddata_sl = data4D.reshape(sl_mask.shape[0] * sl_mask.shape[1] * sl_mask.shape[2], data[0].shape[3]).T        \n",
    "        clf = SVC(kernel='linear', C=1)\n",
    "        scores = cross_val_score(clf, bolddata_sl, labels, cv=3)\n",
    "        accuracy.append(scores.mean())   \n",
    "    return np.array(accuracy)\n",
    "\n",
    "# Execute searchlight on 3 subjects.\n",
    "print(\"Begin Searchlight\\n\")\n",
    "sl_result_3subj = sl.run_searchlight(calc_svm, pool_size=pool_size)\n",
    "print(\"End Searchlight\\n\")\n",
    "end_time = time.time()\n",
    "print(f'Total searchlight duration (including start up time): {np.round((end_time - begin_time),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:**\n",
    "\n",
    "To make sure that each subject is being processed correctly, we can compare the results by loading one subject alone vs. loading the same subject amongst three subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it on the first subject alone\n",
    "data_1 = [all_bold[0]]\n",
    "bcvar_1 = [all_labels[0]]\n",
    "begin_time = time.time()\n",
    "\n",
    "# Create the searchlight object\n",
    "sl = Searchlight(sl_rad=sl_rad,max_blk_edge=max_blk_edge)\n",
    "\n",
    "# Distribute the information to the searchlights (preparing it to run)\n",
    "sl.distribute(data_1, mask)\n",
    "\n",
    "# Data that is needed for all searchlights is sent to all cores via the sl.broadcast function. In this example, we are sending the labels for classification to all searchlights.\n",
    "sl.broadcast(bcvar_1)\n",
    "\n",
    "# Execute searchlight on the first subject\n",
    "sl_result_1subj = sl.run_searchlight(calc_svm, pool_size=pool_size)\n",
    "end_time = time.time()\n",
    "print(f'Total searchlight duration (including start up time): {np.round((end_time - begin_time),2)}')\n",
    "\n",
    "# Compare the results using 1 subject and 3 subjects.\n",
    "# Extract the two accuracy maps.\n",
    "accu_from_1 = np.concatenate(sl_result_1subj[np.where(small_mask>0)])\n",
    "accu_from_3 = np.array([r[0] for r in sl_result_3subj[np.where(small_mask>0)]])\n",
    "\n",
    "# Print them out to compare.\n",
    "print(f\"Accuracy for each kernel function from one subject:\\n {accu_from_1}\")\n",
    "\n",
    "print(f\"Accuracy for each kernel function from 3 subjects:\\n {accu_from_3}\")\n",
    "\n",
    "# Compare using numpy.\n",
    "print (f'Are the two arrays close: {np.allclose(accu_from_1, accu_from_3)}')\n",
    "\n",
    "# Data format of the accuracy array\n",
    "print (\"Shape of the accuracy array: \" + str(sl_result_3subj.shape))\n",
    "print (\"One element of the accuracy array: \"+ str(np.array(sl_result_3subj[target_voxel]))) # each element has the accuracy for 3 subjecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running searchlight analyses on a cluster<a name=\"submitting_searchlights\"></a>\n",
    "\n",
    "**Note: If you are running this section in a non-cluster environment (shouldn't be the case since the data is on the cluster), the run-time for this section can be quite long. You can make an estimate of the run-time (see [exercise 5](#ex5) above) and plan accordingly.**\n",
    "\n",
    "Running searchlight analyses through notebooks or interactive sessions isn't tractable for real studies. Although the example above ran quickly and without parallelization, we only performed 64 analyses. We are now going to write a script to run a searchlight as a \"batch\" job. To learn how to submit jobs, you need to know a bit about [slurm](https://research.computing.yale.edu/support/hpc/user-guide/slurm), the scheduling system we will be using on Grace.\n",
    "\n",
    "To run a job, a good work flow is to have two scripts: One script that actually does the computation you care about (e.g., a python script like `my_script.py`) and a script that sets up the environment and specifies the job parameters (e.g., a bash script like `submit_my_script.sh`). The environment refers to the modules and packages you need to run your job. The job parameters refer to the partition you are going to use (`-p`), the number of cores (`-n`), the amount of memory (`-m`) and required time (`-t`). To run your job you then call the bash script with something like: `sbatch submit_my_script.sh` from the command line.\n",
    "\n",
    "**Self-study:** Lucky for you we have already written the script needed here, called `submit_searchlight.sh`. This script is written in bash. Please explore this script (inside the `07-searchlight` directory) to get familiar with submitting jobs. It will be very useful for future analyses to customize it for your needs (using a text editor like nano or vim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the bash script for running searchlights\n",
    "!cat 07-searchlight/submit_searchlight.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a job starts, a log file named as `searchlight-%j.out` is automatically created in the same directory as your shell script. You can see any output from your script printed into the log file. You can print out the content of the log file in the terminal by running `cat searchlight-%j.out` from the 07-searchlight directory in the command line. You could also opne the file from the Jupyter Lab file tree. To check the status of your job you should use `squeue --me` at the command line. Sometimes your jobs won't run immediately because they are waiting in the queue. Conversely, don't submit too many jobs or your classmates won't be able to run theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Distributing jobs with parallel computing<a name=\"ranks\"></a>\n",
    "\n",
    "When you parallelize an analysis across multiple cores, this typically means that each core will run the code independently (though there are ways for cores to communicate directly with each other). This means you will load in all of the data on each core. The searchlight function then notices that there are multiple cores running the same job and assigns different pieces of the data to each core for analysis. The message passing interface (MPI), the parallelizing framework used by BrainIAK and most high-performance computing applications, keeps track of each process by assigning it a rank, starting at 0. We provide a brief overview of ranks, cores, and nodes below.\n",
    "\n",
    "<img src=\"imgs/nodes_process.jpg\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "\n",
    "What is a rank? It is a process generated by the application that you are running. In the above figure \"Process 1\" can be considered to be rank=0 and \"Process 2\" rank=1. A rank can use more than one core on the cluster and is managed by MPI. Each rank also uses a part of the memory allocated. The cores and memory are part of the hardware belonging to a \"node\". A node is akin to a server. When you submit a job, you are requesting use of part of the node, or sometimes even the entire node. **Optimizing the memory, and number of cores for your batch job will lead to significant gains in run-time for your programs**.\n",
    "\n",
    "MPI handles all communication between the ranks (or processes). This can occur within a single node, or if you have asked for lots of resources, this can even span multiple nodes. Each process can spawn multiple threads, and Open-MP handles communication between threads.\n",
    "\n",
    "\n",
    "<img src=\"imgs/mpi_openmp.jpg\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "\n",
    "**You do not need to configure any of the above. BrainIAK handles all the parallelization for you.** Just make sure that you have requested the appropriate number of cores and the memory for your job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "If you are running an analysis across 2 cores then some of your computations will be run with rank=0 and some with rank=1. The following commands are used to access MPI information that your job needs.\n",
    "\n",
    "```\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "```\n",
    "\n",
    "For efficient utilization of memory, we typically load the data on as few ranks as possible. If memory is sufficient to load all data on one rank, then we would write someting like this:\n",
    "```\n",
    "if rank==0:\n",
    "    load all my data\n",
    "else:\n",
    "    load labels for all subject\n",
    "    load the mask\n",
    "distribute(pieces of my data to other ranks)\n",
    "run searchlight\n",
    "```\n",
    "\n",
    "The above will avoid loading every subjects data on every rank. **The labels and mask must be loaded on all ranks.** The distribution will allocate only pieces of the data for processing. Again, this is all handled by MPI. In searchlight, the `sl.distribute` methods handles the distribution to different ranks.\n",
    "\n",
    "After the searchlights have all finished, you need to save the data. MPI gathers the results from all of the cores and BrainIAK saves the results.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of searchlights run: {len(sl_result[mask==1])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading data only one rank.\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.rank\n",
    "size = comm.size\n",
    "\n",
    "sub_id = 1\n",
    "# Load data only on rank==0\n",
    "if rank==0:\n",
    "    bold_vol, labels, whole_brain_mask, affine_mat, dimsize = load_fs_data(data_path, sub_id)\n",
    "    data = bold_vol\n",
    "else:\n",
    "    data = None\n",
    "    \n",
    "# Make a mask of a 4 x 4 x 4 region\n",
    "target_voxel = (16,20,16)\n",
    "small_mask = np.zeros(whole_brain_mask.shape)\n",
    "x,y,z = target_voxel\n",
    "small_mask[x-2:x+2, y-2:y+2, z-2:z+2] = 1\n",
    "\n",
    "# Preset the variables\n",
    "\n",
    "mask = small_mask\n",
    "bcvar = labels\n",
    "sl_rad = 1\n",
    "max_blk_edge = 5\n",
    "pool_size = 1\n",
    "\n",
    "\n",
    "# Start the clock to time searchlight\n",
    "begin_time = time.time()\n",
    "\n",
    "# Create the searchlight object\n",
    "sl = Searchlight(sl_rad=sl_rad, max_blk_edge=max_blk_edge)\n",
    "print(\"Setup searchlight inputs\")\n",
    "print(f\"Input data shape: {data.shape}\")\n",
    "print(f\"Input mask shape: {mask.shape}\")\n",
    "\n",
    "# Distribute the information to the searchlights (preparing it to run)\n",
    "sl.distribute([data], mask)\n",
    "\n",
    "# Data that is needed for all searchlights is sent to all cores via the sl.broadcast function. In this example, we are sending the labels for classification to all searchlights.\n",
    "sl.broadcast(bcvar)\n",
    "\n",
    "# Set up the kernel, in this case an SVM\n",
    "def calc_svm(data, sl_mask, myrad, bcvar):  \n",
    "    if np.sum(sl_mask) < 12:\n",
    "        return np.nan\n",
    "    \n",
    "    # Pull out the data\n",
    "    data4D = data[0]\n",
    "    labels = bcvar    \n",
    "    bolddata_sl = data4D.reshape(sl_mask.shape[0] * sl_mask.shape[1] * sl_mask.shape[2], data[0].shape[3]).T    \n",
    "    t1 = time.time()\n",
    "    scores = []\n",
    "    \n",
    "    # outer loop\n",
    "    sp = StratifiedKFold(n_splits=3)\n",
    "    for train, test in sp.split(bolddata_sl, labels):\n",
    "        train_data = bolddata_sl[train, :]\n",
    "        test_data = bolddata_sl[test, :]\n",
    "        train_label = labels[train]\n",
    "        test_label = labels[test]     \n",
    "        # inner loop\n",
    "        sp_train = StratifiedKFold(n_splits=2)\n",
    "        parameters = {'C':[0.01, 0.1, 1, 10]}\n",
    "        inner_clf = GridSearchCV(\n",
    "            SVC(kernel='linear'),\n",
    "            parameters,\n",
    "            cv=sp_train,\n",
    "            return_train_score=True)\n",
    "        inner_clf.fit(train_data, train_label)\n",
    "        # best C from inner loop\n",
    "        C_best = inner_clf.best_params_['C'] \n",
    "        # Train in outer loop\n",
    "        clf = SVC(kernel='linear', C=C_best)\n",
    "        clf.fit(train_data, train_label)\n",
    "        # Test in outer loop\n",
    "        scores.append(clf.score(test_data, test_label))\n",
    "    accuracy = np.mean(scores)\n",
    "    t2 = time.time()    \n",
    "    return accuracy\n",
    "\n",
    "print(\"Begin Searchlight\\n\")\n",
    "sl_result = sl.run_searchlight(calc_svm, pool_size=pool_size)\n",
    "print(\"End Searchlight\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print outputs\n",
    "print(\"Summarize searchlight results\")\n",
    "print(f\"Number of searchlights run: {len(sl_result[mask==1])}\")\n",
    "print(f\"Average accuracy across all searchlights:  {np.nanmean(sl_result[mask==1])}\")\n",
    "print(f'Total searchlight duration (including start up time): {np.round((end_time - begin_time),2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have written a Python script for running searchlights. The file *searchlight.py* loads in number of participants, sets up the searchlight and kernel and then performs the searchlight across the multiple cores that the code is run on. \n",
    "\n",
    "This code performs leave-one-subject-out cross validation (i.e., the testing accuracy of subject 1 is based on the SVM classifier trained on subject 2 and 3; accuracy of subject 2 is based on the SVM trained on subject 1 and 3, etc.) The reason we are doing this cross-validation is that the single subject case as in the previous sections in this notebook is a case of double dipping. We only have one run of data for each subject, so the z-scoring within subject will cause double dipping in the single subject case.\n",
    "\n",
    "These results are saved for each subject separately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:**  <a id=\"ex9\"></a> Run the `run_searchlight.sh` script from within the `07-searchlight` folder using `sbatch run_searchlight.sh` on the command line. Once your analysis has finished (should take <5 mins), load in your averaged searchlight results and plot it with the imported function [plotting](https://nilearn.github.io/dev/plotting/index.html) from nilearn. You will find function [plotting.plot_stat_map](https://nilearn.github.io/dev/modules/generated/nilearn.plotting.plot_stat_map.html#nilearn.plotting.plot_stat_map) useful. However, be aware that some of these functions assume the brain is MNI space. This face-scene dataset is *not* in MNI space, so if you overlay it on the default background the image will be 'outside of the brain.' To visualize this correctly, provide the same image as you're plotting as the parameter `bg_img` into this function.  \n",
    "\n",
    "For comparison, also load in the averaged searchlight results across all 18 subjects we computed for you and plot it. The file is in the  `07-searchlight` folder, called 'avg-18_whole_brain_SL.nii.gz'. Use the same `cut_coords`, `threshold`, and `vmax` across these two plots. You can start from `threshold=0.65`, `cut_coords=(50,-40,10)`, `vmax=1` for a good visualization, but play around with these parameters. Compare the two plots. \n",
    "\n",
    "**Note that if you changed the directory of where the results are being stored, you need to update that here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:**  <a id=\"ex10\"></a> From your results, what do you infer about the discriminability of faces vs. scenes. Is it localized to a particular brain region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"ex11\"></a>\n",
    "**Exercise 11:** How do you figure out which searchlight cube is running on which rank when you run it on cluster? Write a new script called `searchlight_rank.py` to figure this out. Following the `searchlight.py` script as an example, write a new kernel function called `calc_rank` which computes and returns the rank of searchlight cube. *Do not perform any classification in this kernel!* Run it for subject 1 and save the resulting brain map as a nifti file. Then, visualize the brain map as you did above.\n",
    "\n",
    "*Note: to run this script, you must either make a new slurm script to submit your code, or rename the script called by `submit_searchlight.sh`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 12:** How many searchlights were run on rank 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novel contribution:**<a name=\"novel\"></a> be creative and make one new discovery by adding an analysis, visualization, or optimization.\n",
    "\n",
    "Some ideas for novel contribution:  \n",
    "- Change the kernel to RSA and correlate it with a model RDM that distinguishes faces and scenes. Compare the RSA results with the decoding results.\n",
    "- Compute the confusion matrix for each searchlight. You can find useful information about confusion matrix [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "- Run a searchlight, but not on every voxel. Run searchlights only on every third voxel\n",
    "- Run each subject on a different rank. BrainIAK searchlight has a way to do that.\n",
    "- Visualize the accuracy map using surface mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel contribution code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a> \n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook 03/2018  \n",
    "T. Meissner minor edits  \n",
    "H. Zhang preprocessed dataset, add multi-subject section, add multiple exercises, add solutions, other edits  \n",
    "Vineet Bansal provided the MPI diagrams.  \n",
    "David Turner provided extensive input on how to use MPI for efficient running of jobs.  \n",
    "M. Kumar added MPI information and enhanced section contents.  \n",
    "K.A. Norman provided suggestions on the overall content and made edits to this notebook.  \n",
    "C. Ellis implemented updates from cmhn-s19.<br/>\n",
    "X. Li changed nipype function get_data() to get_fdata() since get_data() is deprecated in section 1.1.1      \n",
    "T. Yates made edits for cmhn_s21.  \n",
    "E. Busch edited commenting and hardcoding for cmhn-s22, adjusted for Grace cluster and changed submit scripts in cmhn-s23 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
